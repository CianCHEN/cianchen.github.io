{"meta":{"title":"CianCHEN | with code","subtitle":"分享我所知","description":"世间纷繁,乱花迷人眼,前路漫漫,勿忘初心~","author":"CianCHEN","url":"http://192.168.36.183:6888"},"pages":[{"title":"about","date":"2018-12-23T07:20:44.000Z","updated":"2018-12-23T07:21:13.987Z","comments":true,"path":"about/index.html","permalink":"http://192.168.36.183:6888/about/index.html","excerpt":"","text":"一个测试博客的网站!"},{"title":"categories","date":"2018-12-23T07:07:35.000Z","updated":"2018-12-23T07:09:11.193Z","comments":true,"path":"categories/index.html","permalink":"http://192.168.36.183:6888/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2018-12-23T07:07:21.000Z","updated":"2018-12-23T07:08:34.341Z","comments":true,"path":"tags/index.html","permalink":"http://192.168.36.183:6888/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"xtrabackup 全备/增备和还原测试","slug":"xtrabackup-全备-增备和还原测试","date":"2019-01-08T07:44:29.000Z","updated":"2019-01-09T09:33:00.242Z","comments":true,"path":"2019/01/08/xtrabackup-全备-增备和还原测试/","link":"","permalink":"http://192.168.36.183:6888/2019/01/08/xtrabackup-全备-增备和还原测试/","excerpt":"关于xtrabackup 据官方介绍，xtrabackup是世界上唯一一款开源的能够对Innodb和Xtradb存储引擎进行物理热备的工具。XtraBackup包含两部分：xtrabackup的c程序和innobackupex的Perl脚本；前者主要用于处理InnoDB表的备份；后者是前者的封装，主要包括一些与MySQL服务器的通信和MyISAM表的备份。 而且xtrabackup热备份兼容所有版本的Percona Server和MySQL，它基于流式数据，压缩等进行备份，节省空间。 特点： 备份过程快速可靠. 备份过程不打断正在执行的事务. 节约磁盘空间和流量. 自动备份检验. 还原速度快. xtrabackup 相关的参数可以查看官方Index：1https://www.percona.com/doc/percona-xtrabackup/2.4/genindex.html xtrabackup 用户手册:1https://www.percona.com/doc/percona-xtrabackup/2.4/manual.html 根据需求下载xtrabackup1https://www.percona.com/downloads/XtraBackup/LATEST/ 安装xtrabackup我线上的环境用的是5.7.18的mysql，我选择2.4.libgcrypt145版本下载xtrabackup二进制包12# cd /usr/local/src# wget https://www.percona.com/downloads/XtraBackup/Percona-XtraBackup-2.4.12/binary/tarball/percona-xtrabackup-2.4.12-Linux-x86_64.libgcrypt145.tar.gz 解压文件1# tar xf percona-xtrabackup-2.4.12-Linux-x86_64.libgcrypt145.tar.gz","text":"关于xtrabackup 据官方介绍，xtrabackup是世界上唯一一款开源的能够对Innodb和Xtradb存储引擎进行物理热备的工具。XtraBackup包含两部分：xtrabackup的c程序和innobackupex的Perl脚本；前者主要用于处理InnoDB表的备份；后者是前者的封装，主要包括一些与MySQL服务器的通信和MyISAM表的备份。 而且xtrabackup热备份兼容所有版本的Percona Server和MySQL，它基于流式数据，压缩等进行备份，节省空间。 特点： 备份过程快速可靠. 备份过程不打断正在执行的事务. 节约磁盘空间和流量. 自动备份检验. 还原速度快. xtrabackup 相关的参数可以查看官方Index：1https://www.percona.com/doc/percona-xtrabackup/2.4/genindex.html xtrabackup 用户手册:1https://www.percona.com/doc/percona-xtrabackup/2.4/manual.html 根据需求下载xtrabackup1https://www.percona.com/downloads/XtraBackup/LATEST/ 安装xtrabackup我线上的环境用的是5.7.18的mysql，我选择2.4.libgcrypt145版本下载xtrabackup二进制包12# cd /usr/local/src# wget https://www.percona.com/downloads/XtraBackup/Percona-XtraBackup-2.4.12/binary/tarball/percona-xtrabackup-2.4.12-Linux-x86_64.libgcrypt145.tar.gz 解压文件1# tar xf percona-xtrabackup-2.4.12-Linux-x86_64.libgcrypt145.tar.gz 方便管理，重命名一下123# mv percona-xtrabackup-2.4.12-Linux-x86_64 xtrabackup24# ls /usr/local/src/xtrabackup24/bin/innobackupex xbcloud xbcloud_osenv xbcrypt xbstream xtrabackup 创建相关的软链，不用设置环境变量1# ln -sv /usr/local/src/xtrabackup24/bin/* /usr/bin/ 查看innobackupex版本信息，确认安装1# innobackupex -v 全量备份和还原测试查看一下数据库的原始数据123456789101112131415161718192021222324252627282930313233343536logindb --&gt;root@slave-3 [(none)]&gt;show databases;+--------------------+| Database |+--------------------+| information_schema || flask || mysql || performance_schema || sys || txdq_center |+--------------------+6 rows in set (0.00 sec)root@slave-3 [(none)]&gt;use flaskDatabase changedroot@slave-3 [flask]&gt;show tables;+-----------------+| Tables_in_flask |+-----------------+| mytb || tb2 |+-----------------+2 rows in set (0.00 sec)root@slave-3 [flask]&gt;select * from mytb;+----+----------+------+| id | username | core |+----+----------+------+| 1 | john | 98 || 2 | keth | 88 || 3 | mary | 95 || 4 | ben | 93 || 5 | kitty | 68 |+----+----------+------+5 rows in set (0.00 sec) 这里我针对flask库和它下面的mytb表进行测试 创建备份存放目录1# mkdir -p /tmp/xtrabackup/&#123;full,incre&#125; 做一次全量备份1234# innobackupex --defaults-file=/data/mysqlDB/slave/4401/conf/slave.cnf --user=root --password=`cat /data/.dbp` /tmp/xtrabackup/fullxtrabackup: Transaction log of lsn (2832968) to (2832977) was copied.190104 14:44:59 completed OK! 查看第一次的全备123456789101112131415161718192021# ls /tmp/xtrabackup/full/2019-01-04_14-44-55/ -ltotal 282680-rw-r----- 1 root root 488 2019-01-04 14:44:59 backup-my.cnfdrwxr-x--- 2 root root 82 2019-01-04 14:44:58 flask-rw-r----- 1 root root 443 2019-01-04 14:44:59 ib_buffer_pool-rw-r----- 1 root root 134217728 2019-01-04 14:47:34 ibdata1-rw-r----- 1 root root 33554432 2019-01-04 14:47:34 ib_logfile0-rw-r----- 1 root root 33554432 2019-01-04 14:47:32 ib_logfile1-rw-r----- 1 root root 33554432 2019-01-04 14:47:32 ib_logfile2-rw-r----- 1 root root 33554432 2019-01-04 14:47:32 ib_logfile3-rw-r----- 1 root root 12582912 2019-01-04 14:47:33 ibtmp1drwxr-x--- 2 root root 4096 2019-01-04 14:44:58 mysqldrwxr-x--- 2 root root 8192 2019-01-04 14:44:58 performance_schemadrwxr-x--- 2 root root 8192 2019-01-04 14:44:58 sysdrwxr-x--- 2 root root 244 2019-01-04 14:44:58 txdq_center-rw-r----- 1 root root 21 2019-01-04 14:44:58 xtrabackup_binlog_info-rw-r--r-- 1 root root 21 2019-01-04 14:47:30 xtrabackup_binlog_pos_innodb-rw-r----- 1 root root 113 2019-01-04 14:47:32 xtrabackup_checkpoints-rw-r----- 1 root root 534 2019-01-04 14:44:59 xtrabackup_info-rw-r----- 1 root root 8388608 2019-01-04 14:47:32 xtrabackup_logfile-rw-r--r-- 1 root root 1 2019-01-04 14:47:30 xtrabackup_master_key_id 观察一下备份目录里面的文件1234先看一下所有的目录mysql,sys,flask,performance_schema 这些里面都是数据库文件了ibdata1: 独立表空间ib_logfile0-3: redolog 文件，保证数据一致性的ib_buffer_pool: buffer pool 中的热数据，当设置 `innodb_buffer_pool_dump_at_shutdown=1` ，在关闭 MySQL 时，会把内存中的热数据保存在磁盘里 `ib_buffer_pool` 文件中 -&gt; backup-my.cnf, 备份命令使用的参数选项123456789101112131415161718# cat backup-my.cnf # This MySQL options file was generated by innobackupex.# The MySQL server[mysqld]innodb_checksum_algorithm=crc32innodb_log_checksum_algorithm=strict_crc32innodb_data_file_path=ibdata1:128M:autoextendinnodb_log_files_in_group=4innodb_log_file_size=33554432innodb_fast_checksum=falseinnodb_page_size=16384innodb_log_block_size=512innodb_undo_directory=./innodb_undo_tablespaces=0server_id=3redo_log_version=1server_uuid=33768ea9-0f2f-11e9-9277-1e57f2acce04master_key_id=0 -&gt; xtrabackup_binlog_info,mysql服务器当前正在使用的二进制日志文件及至备份这一刻为止二进制日志事件position12# cat xtrabackup_binlog_infomysql-bin.000024 724 xtrabackup_binlog_pos_innodb 这个文件记录的信息跟xtrabackup_binlog_info一样。但是在增量备份的时候这个文件并不存在，这里注意。 -&gt; xtrabackup_checkpoints重点关注一下backup_type,这里是full-backuped1234567# cat xtrabackup_checkpointsbackup_type = full-backupedfrom_lsn = 0to_lsn = 2832968last_lsn = 2832977compact = 0recover_binlog_info = 0 from_lsn: 这里是第一次全备份，所以从0开始to_lsn: 这里是备份完成这一刻所有有效数据的偏移量last_lsn: 最后一个写入重做日志(redo log)的偏移量简而言之，LSN（日志序列号）是重做日志的偏移，to_lsn应与from_lsn配对，主要用于增量备份等。 -&gt; xtrabackup_info是一些数据记录，还原的时候会用到1234567891011121314151617181920# cat xtrabackup_infouuid = 412d5d7c-0fec-11e9-9277-1e57f2acce04name = tool_name = innobackupextool_command = --defaults-file=/data/mysqlDB/slave/4401/conf/slave.cnf --user=root --password=... /tmp/xtrabackup/fulltool_version = 2.4.12ibbackup_version = 2.4.12server_version = 5.7.18-logstart_time = 2019-01-04 14:44:55end_time = 2019-01-04 14:44:59lock_time = 0binlog_pos = filename &apos;mysql-bin.000024&apos;, position &apos;724&apos;innodb_from_lsn = 0innodb_to_lsn = 2832968partial = Nincremental = Nformat = filecompact = Ncompressed = Nencrypted = N -&gt; xtrabackup_logfile 文件是xtrabackup的数据文件，用来恢复前回滚redo log的。 模拟删除数据库12root@slave-3 [flask]&gt;drop database flask;Query OK, 2 rows affected (0.11 sec) 关闭mysql数据库：1# systemctl stop mysqld 模拟恢复数据库：先apply-log，用于恢复前回滚事务1234# innobackupex --apply-log /tmp/xtrabackup/full/2019-01-04_14-44-55/InnoDB: Shutdown completed; log sequence number 2833448190104 14:47:34 completed OK! 注意一下状态变化，这里如果没有把握一次性还原成功的话，必须先备份你的备份数据，否则apply-log之后，备份就改变了。1234567# cat xtrabackup_checkpointsbackup_type = full-backuped# cat xtrabackup_checkpointsbackup_type = full-prepared可以看到backup_type 由原来的 `full-backuped` 变成 `full-prepared` 清空原来mysql的数据目录1# rm -rf data/* 还原数据库1# innobackupex --defaults-file=/data/mysqlDB/slave/4401/conf/slave.cnf --copy-back --rsync /tmp/xtrabackup/full/2019-01-04_14-44-55/ 修改数据目录权限1# chown mysql. -R data 启动mysql检查数据1# systemctl start mysqld 记得先关闭mysql服务并且删除(移走)原来的数据目录,否则还原报。记得修改数据目录的权限,否则mysql启动失败。 登录检查数据12345678910111213root@slave-3 [(none)]&gt;use flaskDatabase changedroot@slave-3 [flask]&gt;select * from mytb;+----+----------+------+| id | username | core |+----+----------+------+| 1 | john | 98 || 2 | keth | 88 || 3 | mary | 95 || 4 | ben | 93 || 5 | kitty | 68 |+----+----------+------+5 rows in set (0.03 sec) 全量备份和还原成功,数据已经回来。 增量备份恢复数据之后先进行一次全备123# innobackupex --defaults-file=/data/mysqlDB/slave/4401/conf/slave.cnf --user=root --password=`cat /data/.dbp` /tmp/xtrabackup/fullxtrabackup: Transaction log of lsn (2833467) to (2833476) was copied.190104 15:13:32 completed OK! 这里注意两个时间点2019-01-04_14-44-55是第一次全备的，2019-01-04_15-13-29这个是恢复数据之后重新做的一个全备，后面的增量备份全部基于这个全备时间点12# ls /tmp/xtrabackup/full2019-01-04_14-44-55 2019-01-04_15-13-29 为了体现效果，这里新建一个新表testtb，并且插入一些测试数据 第一次数据修改12345678910111213141516root@slave-3 [flask]&gt;create table testtb (id int, name varchar(20) NOT NULL);Query OK, 0 rows affected (0.04 sec)root@slave-3 [flask]&gt;INSERT INTO `testtb` VALUES (1,&apos;a&apos;),(2,&apos;b&apos;),(3,&apos;c&apos;);Query OK, 3 rows affected (0.01 sec)Records: 3 Duplicates: 0 Warnings: 0root@slave-3 [flask]&gt;select * from testtb;+------+------+| id | name |+------+------+| 1 | a || 2 | b || 3 | c |+------+------+3 rows in set (0.00 sec) 基于上一次的全量备份进行 第一次增量备份123# innobackupex --defaults-file=/data/mysqlDB/slave/4401/conf/slave.cnf --user=root --password=`cat /data/.dbp` --incremental /tmp/xtrabackup/incre --incremental-basedir=/tmp/xtrabackup/full/2019-01-04_15-13-29/xtrabackup: Transaction log of lsn (2839777) to (2839786) was copied.190104 15:32:18 completed OK! –incremental 指定备份为增量备份–incremental-basedir 指定增量备份的basedir，就是上边的全备时间点文件 12# ls incre2019-01-04_15-33-58 第二次修改数据1234567891011root@slave-3 [flask]&gt;delete from testtb where id =2;Query OK, 1 row affected (0.03 sec)root@slave-3 [flask]&gt;select * from testtb;+------+------+| id | name |+------+------+| 1 | a || 3 | c |+------+------+2 rows in set (0.00 sec) 第二次增量备份123# innobackupex --defaults-file=/data/mysqlDB/slave/4401/conf/slave.cnf --user=root --password=`cat /data/.dbp` --incremental /tmp/xtrabackup/incre --incremental-basedir=/tmp/xtrabackup/incre/2019-01-04_15-33-58 --parallel=2 xtrabackup: Transaction log of lsn (2840255) to (2840264) was copied.190104 15:38:31 completed OK! 模拟误操作删除数据12root@slave-3 [flask]&gt;drop table testtb;Query OK, 0 rows affected (0.01 sec) 增量备份还原准备还原数据增量备份之前记得redo-only，用于回滚未同步事务以及未提交事务123# innobackupex --apply-log --redo-only /tmp/xtrabackup/full/2019-01-04_15-13-29/# cat /tmp/xtrabackup/full/2019-01-04_15-13-29/xtrabackup_checkpoints backup_type = log-applied //状态变了，redo-only 第一次增备合并12345# innobackupex --apply-log --redo-only /tmp/xtrabackup/full/2019-01-04_15-13-29 --incremental-dir=incre/2019-01-04_15-33-58190104 15:43:32 completed OK!# cat /tmp/xtrabackup/full/2019-01-04_15-13-29/xtrabackup_checkpoints backup_type = log-applied 第二次增备合并123456# innobackupex --apply-log /tmp/xtrabackup/full/2019-01-04_15-13-29/ --incremental-dir=incre/2019-01-04_15-38-25InnoDB: Shutdown completed; log sequence number 2840616190104 15:44:28 completed OK!# cat /tmp/xtrabackup/full/2019-01-04_15-13-29/xtrabackup_checkpoints backup_type = full-prepared 注意，最后一个增量备份合并的时候不需要--redo-only，而且backup_type已经变成full-prepared 完整备份apply123# innobackupex --apply-log /tmp/xtrabackup/full/2019-01-04_15-13-29/InnoDB: Shutdown completed; log sequence number 2840654190104 15:45:21 completed OK! 这样就准备好还原的数据了 应用还原数据关闭mysql1# systemctl stop mysqld 清空原来mysql的数据目录1# rm -rf data/* 还原数据库12# innobackupex --defaults-file=/data/mysqlDB/slave/4401/conf/slave.cnf --copy-back --rsync /tmp/xtrabackup/full/2019-01-04_15-13-29/190104 15:49:33 completed OK! 修改数据目录权限1# chown mysql. -R data 启动mysql1# systemctl start mysqld 登录检查数据12345678910root@slave-3 [(none)]&gt;use flaskDatabase changedroot@slave-3 [flask]&gt;select * from testtb;+------+------+| id | name |+------+------+| 1 | a || 3 | c |+------+------+2 rows in set (0.22 sec) 可以看到数据已经恢复了。","categories":[],"tags":[{"name":"xtrabackup","slug":"xtrabackup","permalink":"http://192.168.36.183:6888/tags/xtrabackup/"}]},{"title":"open-falcon之告警篇","slug":"open-falcon之告警篇","date":"2018-12-25T06:38:04.000Z","updated":"2018-12-26T05:36:58.756Z","comments":true,"path":"2018/12/25/open-falcon之告警篇/","link":"","permalink":"http://192.168.36.183:6888/2018/12/25/open-falcon之告警篇/","excerpt":"在上一篇博文中，已经演示了怎样部署一套基础的监控服务端，下面要开始安装agent收集指标信息以及配置告警，实现监控基础环境信息：12server端: 192.168.36.183agent: 192.168.36.17 编译安装agent编译agent客户端agent，我们只需要编译一次即可，以后所有的agent都可以直接使用这个包了。在server端机器进行客户端编译：1234cd /opt/go/src/github.com/open-falcon/falcon-plus/modules/agent #这个目录之前在编译server的时候使用过go get ./... #需要主机可以连接外网，通过go get下载相关源码包。./control build./control pack #编译pack 出的包，在其他agent主机上部署时，无需连接外网 ，pack出的包，可以类似的理解为由c源代码编译后得出的二进制文件。 观察一下我们需要的压缩包falcon-agent-5.1.2.tar.gz 部署agent","text":"在上一篇博文中，已经演示了怎样部署一套基础的监控服务端，下面要开始安装agent收集指标信息以及配置告警，实现监控基础环境信息：12server端: 192.168.36.183agent: 192.168.36.17 编译安装agent编译agent客户端agent，我们只需要编译一次即可，以后所有的agent都可以直接使用这个包了。在server端机器进行客户端编译：1234cd /opt/go/src/github.com/open-falcon/falcon-plus/modules/agent #这个目录之前在编译server的时候使用过go get ./... #需要主机可以连接外网，通过go get下载相关源码包。./control build./control pack #编译pack 出的包，在其他agent主机上部署时，无需连接外网 ，pack出的包，可以类似的理解为由c源代码编译后得出的二进制文件。 观察一下我们需要的压缩包falcon-agent-5.1.2.tar.gz 部署agent 将包发送到客户机器上，解压123mkdir -p /usr/local/open-falcon/agentcd /usr/local/open-falcon/agenttar -xf falcon-agent-5.1.2.tar.gz -C ./ 修改相关的配置1234567891011cp cfg.example.json cfg.jsonip --&gt; agent会自动探测本机iphostname --&gt; 默认通过`hostname`获取,如果你自定义了就会覆盖，这个在dashboard显示的就是endpoint heartbeat[addr] --&gt; 192.168.36.183ignore --&gt; 选择你要忽略的监控项# 这里基本选择默认就行，按需修改，这里注意一下1988端口，这个是会暴露agent机器的所有指标信息，建议改成localhost&quot;http&quot;: &#123; &quot;enabled&quot;: true, &quot;listen&quot;: &quot;127.0.0.1:1988&quot;, &quot;backdoor&quot;: false &#125;, 开启falcon-agent./falcon-agent start检查falcon-agent是否正常./falcon-agent -check 通过访问http://192.168.36.17:1988/ 可以看到agent本机的所有详细信息，一般这个不建议监听可访问地址，上边讲过。 接着登录dashbord就可以看见新加的agentd了相关的监控信息也会出来，按需查看即可 配置告警：编译安装mail-provider这里务必注意了，告警需要alarm插件的调用，而alarm需要redis，所以必须启动redis，否则报警不成功如果之前启动server的时候没有启动redis，那么启动redis之后要记得重启一下server，否则一大堆问题。 open-falcon实现邮件报警$GOPATH –&gt; /opt/go 在服务端机器执行以下的步骤，告警是放在server端的以下操作在server端执行 编译mail-provider123456789cd $GOPATH/srccd github.com/open-falcon/# 下载mail-provider源码git clone https://github.com/open-falcon/mail-provider.gitcd mail-provider/# 安装go依赖go get ./..../control build./control pack 安装mail-provider1234567891011121314151617181920mkdir /opt/open-falcon/mail-providertar zxf falcon-mail-provider-0.0.1.tar.gz -C /opt/open-falcon/mail-provider/cd /opt/open-falcon/mail-provider/vim cfg.json #修改一下配置文件，邮件相关的信息&#123; &quot;debug&quot;: true, &quot;http&quot;: &#123; &quot;listen&quot;: &quot;0.0.0.0:4000&quot;, &quot;token&quot;: &quot;&quot; &#125;, &quot;smtp&quot;: &#123; &quot;addr&quot;: &quot;smtp.163.com:25&quot;, &quot;username&quot;: &quot;yourname@163.com&quot;, &quot;password&quot;: &quot;password@123&quot;, &quot;from&quot;: &quot;yourname163.com&quot; &#125;&#125;# 开启mail-provider./control start 测试邮件发送：1curl http://127.0.0.1:4000/sender/mail -d &quot;tos=1653581786@qq.com&amp;subject=报警测试&amp;content=这是一封测试邮件&quot; 发送邮件测试一下 –&gt; 返回success成功 需要实现告警还需要一个sender组件:上边的邮件发送插件可以用你们自己开发的接口或者三方工具都行，sender使用post方式触发 编译安装sender编译sender:12345678https://github.com/open-falcon-archive/sendercd $GOPATH/srccd github.com/open-falcon/git clone https://github.com/open-falcon/sender.gitcd sender/go get ./..../control build./control pack 安装sender123mkdir /opt/open-falcon/sendertar zxf falcon-sender-0.0.0.tar.gz -C /opt/open-falcon/sender/cd /opt/open-falcon/sender/ 修改一下配置文件：12vim cfg.json 就是将mail那里修改成http://127.0.0.1:4000/sender/mail 启动sender1./control start 到这里基础的环境就准备的差不多了，接下来去添加一下监控和告警条件 配置监控和告警需要注意，open-falcon接受告警不是单个用户的，需要定义一个告警接收组，这里区别于zabbix 用户/组管理 创建告警组 创建HostGrouphostgroup 的作用很简单，就是我需要跑的程序都是在某些机器上，那我直接把这些机器都加入到同一个hostgroup即可，不用每台机器都添加一个模板了 创建策略模板这里all(#1)==0 为告警触发条件，意思是最后一次返回的值等于0就告警具体的参考:http://book.open-falcon.org/zh_0_2/distributed_install/judge.html 绑定策略模板到HostGroup 这样简单的监控就算是可以实现了 保证server端redis正常的话，关闭一下agentd的6379端口，监听一下日志1tailf var/app.log #可以查看下日志可以看到提示邮件已经发出去了 报警间隔自己修改了，默认5min12vim /usr/local/open-falcon/judge/config/cfg.json #配置文件中配置了连续两个报警之间至少相隔的秒数&quot;minInterval&quot;: 300, 告警信息此时在alarm-dashbord也应该出现告警信息。 配置nodatahttps://github.com/open-falcon-archive/nodata这个东西就是监控项在没有数据返回的时候，返回一个自定义的值，比如agentd挂了，就没办法上报数据了，这时候nodata就发挥作用了，我自定义一个-1，告诉运维，有agentd挂了。 新建nodata 到dashboard上查看相应的counter 查看一下对应的出图 可以观察到，这个新建的counter由nodata创建是成功的，但是返回的值确实一直是异常的返回，也就是我们自定义的-1 监控原生的那个agent.alive，他是没有标签的，所以nodata那里配置不应该打上tag.当我们按照官方教程上那么设置tag之后，nodata就会去监控agent.alive/module=nodata,pdl=falcon这一项，这一Counter本来就没有数据，是nodata自己创建的。所以nodata每次都去获取agent.alive/module=nodata,pdl=falcon这项数据，发现为空，返回-1，所以这一项肯定是永远为-1的。 修改nodata接下来到nodata上去掉一开始打的tags nodata实现告警跟上边告警一样，我新建一个告警模板然后将模板绑定到hostgroup上 测试nodata告警我关闭36.17上的agent进程./control stop dashboard上agent.avlie counter观察一下出图注意这里是agent.avlie而不再是agent.alive/module=nodata,pdl=falcon了，这个带tags的counter已经不会采集数据，你可以在界面删除它可以看到，agent的值从1到-1，异常，将会触发告警，我这里会有邮件触发。 告警alarm-dashbord到这里实现了简单的告警,以及策略的编写和配置了nodata,sender。下一篇将会是简单应用监控。","categories":[{"name":"open-falcon","slug":"open-falcon","permalink":"http://192.168.36.183:6888/categories/open-falcon/"}],"tags":[{"name":"open-falcon","slug":"open-falcon","permalink":"http://192.168.36.183:6888/tags/open-falcon/"}]},{"title":"open-falcon之安装篇","slug":"open-falcon之安装篇","date":"2018-12-24T08:31:08.000Z","updated":"2018-12-25T06:18:46.586Z","comments":true,"path":"2018/12/24/open-falcon之安装篇/","link":"","permalink":"http://192.168.36.183:6888/2018/12/24/open-falcon之安装篇/","excerpt":"小米开源监控软件 - open-falcon 安装篇 open-falcon官网地址：http://open-falcon.org/ 相关的文档地址(v2.0)：https://book.open-falcon.org/zh_0_2/intro/ 官网的架构示意图 这里尽量以我的理解来说一下这个架构图每个组件的作用(大部分来自官网)： falcon-agent是部署在每一台被监控的主机上的，主要作用是负责数据采集和数据上报，另外它还有一个proxy-gateway的接口提供给用户使用，这样用户就可以自定义一些数据，然后上报给agent，agent再上报到transfer transfer是一个无状态的集群(可单点)。transfer接收agent上报的数据，然后使用一致性哈希进行数据分片、并把分片后的数据转发给graph、judge集群(transfer还会打一份数据到opentsdb)。其消耗的资源 主要是网络和CPU transfer辐射的一个组件是graph，graph组件用于存储、归档作图数据，可以集群部署。每个graph实例会处理一个分片的数据: 接收transfer发送来的分片数据，归档、存储、生成索引；接受query对该数据分片的查询请求。graph会持久存储监控数据，频繁写入磁盘，状态数据会缓存在内存，因此graph消耗的主要资源是磁盘存储、磁盘IO和内存资源。 图中API的用途是处理终端用户请求，收到查询请求后，会去多个graph里面，查询不同metric的数据，汇总后统一返回给用户。 这里顺便说一下dashboard，是一个可视化的用户管理ui，用户可以以多个维度来搜索endpoint列表，即可以根据上报的tags来搜索关联的endpoint。 Aggregato 的作用是聚合监控数据，比如说多个机器的某个指标，提供给dashboard显示 nodata 组件的作用是检测监控数据的上报异常，比如agent.alived，异常可以返回-1，重新push到transfer，从而触发告警。 tranfer辐射的还有一个叫judge的插件，judge用于实现报警策略的触发逻辑。judge实现触发计算时，会在本地缓存 触发逻辑的中间状态和定量的监控历史数据，因此会消耗较多的内存资源和计算资源。 告警judge触发需要一些触发条件，这里需要heartbeat server的策略下发，hbs是open-falcon的配置中心，负责 适配系统的配置信息、管理agent信息等。 这样judge收到告警策略(portal ui组件配置)之后进行告警判断，符合规则的就放到redis队列中，然后视配置情况进行告警或者合并告警(links组件)，告警使用的是alarm组件，alarm需要调用sender(email, 短信api…)","text":"小米开源监控软件 - open-falcon 安装篇 open-falcon官网地址：http://open-falcon.org/ 相关的文档地址(v2.0)：https://book.open-falcon.org/zh_0_2/intro/ 官网的架构示意图 这里尽量以我的理解来说一下这个架构图每个组件的作用(大部分来自官网)： falcon-agent是部署在每一台被监控的主机上的，主要作用是负责数据采集和数据上报，另外它还有一个proxy-gateway的接口提供给用户使用，这样用户就可以自定义一些数据，然后上报给agent，agent再上报到transfer transfer是一个无状态的集群(可单点)。transfer接收agent上报的数据，然后使用一致性哈希进行数据分片、并把分片后的数据转发给graph、judge集群(transfer还会打一份数据到opentsdb)。其消耗的资源 主要是网络和CPU transfer辐射的一个组件是graph，graph组件用于存储、归档作图数据，可以集群部署。每个graph实例会处理一个分片的数据: 接收transfer发送来的分片数据，归档、存储、生成索引；接受query对该数据分片的查询请求。graph会持久存储监控数据，频繁写入磁盘，状态数据会缓存在内存，因此graph消耗的主要资源是磁盘存储、磁盘IO和内存资源。 图中API的用途是处理终端用户请求，收到查询请求后，会去多个graph里面，查询不同metric的数据，汇总后统一返回给用户。 这里顺便说一下dashboard，是一个可视化的用户管理ui，用户可以以多个维度来搜索endpoint列表，即可以根据上报的tags来搜索关联的endpoint。 Aggregato 的作用是聚合监控数据，比如说多个机器的某个指标，提供给dashboard显示 nodata 组件的作用是检测监控数据的上报异常，比如agent.alived，异常可以返回-1，重新push到transfer，从而触发告警。 tranfer辐射的还有一个叫judge的插件，judge用于实现报警策略的触发逻辑。judge实现触发计算时，会在本地缓存 触发逻辑的中间状态和定量的监控历史数据，因此会消耗较多的内存资源和计算资源。 告警judge触发需要一些触发条件，这里需要heartbeat server的策略下发，hbs是open-falcon的配置中心，负责 适配系统的配置信息、管理agent信息等。 这样judge收到告警策略(portal ui组件配置)之后进行告警判断，符合规则的就放到redis队列中，然后视配置情况进行告警或者合并告警(links组件)，告警使用的是alarm组件，alarm需要调用sender(email, 短信api…) https://book.open-falcon.org/zh_0_2/practice/deploy.html 编译安装后端这里我机器的原因我只会进行单机的安装，有条件的可以实现以下集群安装：安装环境：12345OS：CentOS Linux release 7.3.1611 (Core)go version： go1.9.4 linux/amd64git version： 1.8.3.1redis： 3.2.10mysql：5.7.18 这里我的mysql是源码安装以外，其它都使用yum安装方式。yum install git golang redis 基础环境配置之后，设置一下GOPATH环境变量123vim ~/.bashrcexport GOPATH=/opt/gosource ~/.bashrc #这个是自定义的，可以理解成你放go代码的地方，通常地下还需要有src(放源码)，pkg，bin 三个目录，而这个路径又叫做workspace #这里务必写对，除了GOPATH是你自定义的，其他请按照这个来，不然编译的时候会出错 123mkdir -p $GOPATH/src/github.com/open-falconcd $GOPATH/src/github.com/open-falcongit clone https://github.com/open-falcon/falcon-plus.git 创建需要的数据库先来看看需要导入的5个sql文件导入到mysql数据库1234cd $GOPATH/src/github.com/open-falcon/falcon-plus/scripts/mysql/db_schemafor i in \\`ls ./\\` ;​ do mysql -uroot -P4400 -h127.0.0.1 -p123456 &lt; $i;done 创建编译后的文件地址mkdir /opt/open-falcon 编译open-falcon后端服务1234cd $GOPATH/src/github.com/open-falcon/falcon-plus/make allmake agentmake pack 执行成功出现下边这个压缩包： open-falcon-v0.2.1.tar.gz 1tar -xf open-falcon-v0.2.1.tar.gz -C /opt/open-falcon/ 修改mysql的端口和密码，否则启动失败123cd /opt/open-falcongrep -Ilr 3306 ./ | xargs -n1 -- sed -i &apos;s/root:/root:123456/g&apos;grep -Ilr 3306 ./ | xargs -n1 -- sed -i &apos;s/3306/4400/g&apos; 启动后端：./open-falcon start观察一下成功输出 检查后端：./open-falcon check观察一下check输出 安装前端选择性安装依赖：12yum install -y python-virtualenv python-devel openldap-devel mysql-develyum groupinstall &quot;Development tools&quot; 安装dashboard12cd /opt/open-falcongit clone https://github.com/open-falcon/dashboard.git 创建一个虚拟python环境env12cd dashboardvirtualenv ./env 使用虚拟环境安装python依赖，独立于宿主机1./env/bin/pip install -r pip_requirements.txt -i https://pypi.douban.com/simple 修改一下rrd的配置文件，主要是mysql的配置，其他按需修改1234567vim rrd/config.pyPORTAL_DB_HOST = os.environ.get(&quot;PORTAL_DB_HOST&quot;,&quot;127.0.0.1&quot;)PORTAL_DB_PORT = int(os.environ.get(&quot;PORTAL_DB_PORT&quot;,4400))PORTAL_DB_USER = os.environ.get(&quot;PORTAL_DB_USER&quot;,&quot;root&quot;)PORTAL_DB_PASS = os.environ.get(&quot;PORTAL_DB_PASS&quot;,&quot;123456&quot;)API_ADDR = os.environ.get(&quot;API_ADDR&quot;,&quot;http://192.168.36.183:8080/api/v1&quot;) //这个修改成外网的ip 启动前端：./env/bin/python wsgi.py #这是以开发者模式启动，将会阻塞终端或者./control start #以生产环境启动，另外./control stop就是停止dashboard 查看日志./control tail #查看日志 其他各相关的日志在各自模块下的log文件夹下 使用的相关端口[root@int-test-1 dashboard]# netstat -tunpl | grep fa tcp6 0 0 :::6055 :::* LISTEN 7166/falcon-aggrega tcp6 0 0 :::6090 :::* LISTEN 7158/falcon-nodata tcp6 0 0 :::6060 :::* LISTEN 7552/falcon-transfe tcp6 0 0 :::14444 :::* LISTEN 7186/falcon-gateway tcp6 0 0 :::6030 :::* LISTEN 7136/falcon-hbs tcp6 0 0 :::6031 :::* LISTEN 7136/falcon-hbs tcp6 0 0 :::8080 :::* LISTEN 7195/falcon-api tcp6 0 0 :::8433 :::* LISTEN 7552/falcon-transfe tcp6 0 0 :::6070 :::* LISTEN 7122/falcon-graph tcp6 0 0 :::6071 :::* LISTEN 7122/falcon-graph tcp6 0 0 :::9912 :::* LISTEN 7209/falcon-alarm tcp6 0 0 :::3000 :::* LISTEN 994/grafana-server tcp6 0 0 :::4444 :::* LISTEN 7552/falcon-transfe tcp6 0 0 :::16060 :::* LISTEN 7186/falcon-gateway tcp6 0 0 :::6080 :::* LISTEN 7147/falcon-judge tcp6 0 0 :::18433 :::* LISTEN 7186/falcon-gateway tcp6 0 0 :::6081 :::* LISTEN 7147/falcon-judge tcp6 0 0 :::1988 :::* LISTEN 7176/falcon-agent tcp 0 0 0.0.0.0:8081 0.0.0.0:* LISTEN 10660/python 8081端口是dashboard的端口，访问前端：http://192.168.36.183:8081/ 使用dashboard登录之后需要先注册一个账号 填写相关的信息，注意邮箱地址，告警的时候会使用到 登录之后可以修改用户信息 查看dashboard 查看/过滤 endpoint 和counter 查看监控图像 好了，到这里就可以简单的查看监控的counter和监控图像了，接下来一篇会部署agent到其他节点，部署告警策略。","categories":[{"name":"open-falcon","slug":"open-falcon","permalink":"http://192.168.36.183:6888/categories/open-falcon/"}],"tags":[{"name":"open-falcon","slug":"open-falcon","permalink":"http://192.168.36.183:6888/tags/open-falcon/"}]},{"title":"Zabbix LLD Discovery - zabbix监控之低等级发现","slug":"Zabbix-LLD-Discovery-zabbix监控之低等级发现","date":"2018-12-21T07:12:03.000Z","updated":"2018-12-24T03:32:43.050Z","comments":true,"path":"2018/12/21/Zabbix-LLD-Discovery-zabbix监控之低等级发现/","link":"","permalink":"http://192.168.36.183:6888/2018/12/21/Zabbix-LLD-Discovery-zabbix监控之低等级发现/","excerpt":"zabbix 的Regular expressions以下是一个zabbix文件系统自动发现的正则表达式，匹配到的话返回true：下面看一下文件系统自动发现的具体配置：在zabbix server通过zabbix_get获取一下文件系统发现的全部key：","text":"zabbix 的Regular expressions以下是一个zabbix文件系统自动发现的正则表达式，匹配到的话返回true：下面看一下文件系统自动发现的具体配置：在zabbix server通过zabbix_get获取一下文件系统发现的全部key：可以观察到，这里匹配{ #FSTYPE }的这个key，必须要符合上述正则表达式(返回True)才会返回，这个是zabbix自带的自动发现。再说说这个Filters的作用：实际上就是一个限制作用，只有符合我们自定义的规则，才会返回到zabbix的监控项，否则就不返回，这个看个人具体需求，适合一些已知的监控项，比如文件系统，网卡类型等。如果是不可控的可变类型，建议不要使用，会导致匹配失败而不能发现监控项。 监控实例单监控key实例1 监控mysql是否在线新建监控模板通常我的做法是新加一个lld规则，新建一个模板，填上模板名称即可 新建应用(aapplication)我这里把下一个实例的应用也同时创建了 添加监控脚本这一步是重点，通过自定义的脚本来为zabbix添加自定义key脚本输出必须为标准的json格式，至于用py 还是 shell自己选择，我是觉得系统数据获取还是shell比较方便修改zabbix_agentd.conf重启zabbix_agentd 就可以获取对应添加的key了，下面测试一下mysql.alived 就是添加的自定义key，可以正常返回获取对应{ #MYSQL_PORT } 端口的值：zabbix_get -s 192.168.236.32 -k &quot;mysql.alive[5500]&quot;以上测试通过之后可以进行下一步，这里注意脚本的权限，zabbix用户务必具有执行权限。 配置zabbix自动发现在zabbix面板新建两个discovery rules 配置discovery rule，这里Filters暂时不使用 为模板添加items 配置items 创建报警触发器 配置触发器我这里是正常返会1，异常返回0。当最后一次的值小于1时触发告警，告警级别为Disaster。 创建监控图表 配置监控图表以上就完成一个lld 自动发现模板的配置了。 监控测试进入到套用此模板的服务器，查看自动发现item是否已经正常加入监控。我们过滤一下mysql_alived这个application，可以看到监控项正常加入 测试告警可以关闭监控mysql进行。实例2 监控mysql主从(一从多主)关系是否正常参照2.1的配置就行了，就是修改一下key的名称即可。多监控key实例1 监控mysql主从(一从多主)延时监控脚本返回这个的步骤其实跟上边也是差不多的，先来看看监控返回格式这里其实就跟一开始我们分析的文件系统自动返现那里是类似的。跟上例子不一样的是，这里使用了两个json key作为一个分组，分别是{ #MYSQL_PORT } { #CHANNEL_NAME },我们现在要实现的是监控这些端口下所有channel的mysql主从同步的 Second_behind_master参数，所以我们必须先得到所有的mysql port 和 每个port 下对应的channel名称才能得到Second_behind_master的值。 这里mysql.behind 也是自定义的监控key，在zabbix_agentd.conf进行自定义添加即可。 配置regular expression新建一个regular expression，这里用途是匹配一从多主的channel_name,等下会用到 配置zabbix 自动发现新建一个模板，新建application，新建discovery rule这里filters使用上边刚刚添加的regular expression新建items这里{ #MYSQL_PORT } 和 { #CHANNEL_NAME } 的使用其实就很明确了，你得事先让zabbix知道他们分别代指什么东西，之后zabbix就能将他们作为数据参数来使用了，这就是为什么一开始就要先获取到这两个key的全部数据。 接下来添加触发器和添加图标的操作是一样的：下面是图表添加的范例 监控测试进入到套用模板的机器，过滤对应的application","categories":[{"name":"Zabbix 监控","slug":"Zabbix-监控","permalink":"http://192.168.36.183:6888/categories/Zabbix-监控/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://192.168.36.183:6888/tags/hexo/"},{"name":"zabbix","slug":"zabbix","permalink":"http://192.168.36.183:6888/tags/zabbix/"}]}]}